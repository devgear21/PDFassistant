{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01e295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "nomic_api_key = os.getenv(\"NOMIC_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51dcc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomic import embed, login\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class NomicEmbedText(Embeddings):\n",
    "    def __init__(self, api_key: str):\n",
    "        login(api_key)  # this sets the global token\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        response = embed.text(\n",
    "            texts=texts,\n",
    "            model='nomic-embed-text-v1',\n",
    "        )\n",
    "        return response[\"embeddings\"]\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self.embed_documents([text])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236fd334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aabdu\\AppData\\Local\\Temp\\ipykernel_5100\\2449562798.py:8: LangChainPendingDeprecationWarning: This class is pending deprecation and may be removed in a future version. You can swap to using the `PGVector` implementation in `langchain_postgres`. Please read the guidelines in the doc-string of this class to follow prior to migrating as there are some differences between the implementations. See <https://github.com/langchain-ai/langchain-postgres> for details about the new implementation.\n",
      "  vectorstore = PGVector(\n",
      "C:\\Users\\aabdu\\AppData\\Local\\Temp\\ipykernel_5100\\2449562798.py:8: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  vectorstore = PGVector(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "\n",
    "COLLECTION_NAME = \"pdf_chunks\"\n",
    "CONNECTION_STRING = \"postgresql+psycopg2://ai:ai@localhost:5532/ai\"\n",
    "\n",
    "nomic_embeddings = NomicEmbedText(api_key=nomic_api_key)\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=nomic_embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511f82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def read_and_split_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = splitter.split_text(text)\n",
    "    docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484bdd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aabdu\\AppData\\Local\\Temp\\ipykernel_5100\\2449562798.py:8: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  vectorstore = PGVector(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "\n",
    "COLLECTION_NAME = \"pdf_chunks\"\n",
    "CONNECTION_STRING = \"postgresql+psycopg2://ai:ai@localhost:5532/ai\"\n",
    "\n",
    "nomic_embeddings = NomicEmbedText(api_key=nomic_api_key)\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=nomic_embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f277b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_pdf_in_vectorstore(file_path):\n",
    "    docs = read_and_split_pdf(file_path)\n",
    "    vectorstore.add_documents(docs)\n",
    "    print(f\" Stored {len(docs)} chunks in pgvector.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7a0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stored 192 chunks in pgvector.\n"
     ]
    }
   ],
   "source": [
    "# to store PDF\n",
    "store_pdf_in_vectorstore(\"sample.pdf\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f0107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def read_and_split_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = splitter.split_text(text)\n",
    "    docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e8a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq(api_key=groq_api_key)\n",
    "\n",
    "def ask_llm(question, context, model=\"llama3-70b-8192\"):\n",
    "    prompt = f\"\"\"You are a helpful assistant answering questions based on the provided PDF context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    chat_completion = groq_client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "339e563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_from_pdf(query, top_k=5):\n",
    "    docs = vectorstore.similarity_search(query, k=top_k)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    return ask_llm(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954e012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: What is the easiest dish in this pdf?\n",
      " Answer: I'm happy to help! However, I must point out that there are no specific dish recipes mentioned in the provided PDF context. The context appears to be related to a cooking course or textbook, listing various herbs and spices, and outlining learning objectives for a chapter on introduction to worldwide cuisines.\n",
      "\n",
      "Therefore, it is not possible to identify the easiest dish in this PDF, as there are no dishes mentioned. If you have any further questions or if there's anything else I can assist you with, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the easiest dish in this pdf?\"\n",
    "answer = answer_question_from_pdf(question)\n",
    "\n",
    "print(\" Question:\", question)\n",
    "print(\" Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea78231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
